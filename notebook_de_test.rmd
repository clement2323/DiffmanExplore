---
title: "Diffman tutorial"
author: "Clément G"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
paquets <- c("data.table","prettydoc","devtools","testthat","ggplot2","dplyr","btb")
```

```{r warning=FALSE,message=FALSE,echo = FALSE}
install.packages(paquets)
```

```{r warning=FALSE,message=FALSE,echo = FALSE}
p <- lapply(paquets, function(p) suppressPackageStartupMessages(library(p, character.only = TRUE)))
```

## Installation du package difffman dans diffmanTab
```{r}
remove.packages("diffman")  
devtools::install_github(
  repo = "InseeFrLab/diffman-1",
  ref = "diffmanTab",# la branche
  build_vignettes = TRUE,
  force = TRUE
  )
vignette("diffman")
library(diffman)
```

```{r}
source("fonctions_utiles.R")
```
###  Extraction donnes S3
A copier coller directement dans le terminal (en une fois ça marche pour récupérer les données)

mc cp s3/cguillo/commune_franceentiere_2021.dbf ext_data/commune_franceentiere_2021.dbf
mc cp s3/cguillo/commune_franceentiere_2021.fix ext_data/commune_franceentiere_2021.fix
mc cp s3/cguillo/commune_franceentiere_2021.prj ext_data/commune_franceentiere_2021.prj
mc cp s3/cguillo/commune_franceentiere_2021.shp ext_data/commune_franceentiere_2021.shp
mc cp s3/cguillo/commune_franceentiere_2021.shx ext_data/commune_franceentiere_2021.shx
mc cp s3/cguillo/data_rp.rds ext_data/data_rp.rds
mc cp --recursive s3/cguillo/diff_info/ diff_info
mc cp --recursive s3/cguillo/diff_info_2km/ diff_info_2km

## Code de travail, je pars des données RP
```{r}
### Ce code sera amené à disparaitre, en fire une vignette ! ou bien un rmd !!!
#### Test sur données réelles 
communes <-sf::st_read("ext_data/commune_franceentiere_2021.shp")
data_rp <- data.table(readRDS("ext_data/data_rp.rds"))
compo <- return_connected_components(data_rp)
comp_to_nb_com <- compo[,.(nb_com = length(z1)),by ="id_comp"][rev(order(nb_com)),]
comp_to_nb_com

```

Ici je choisis une composante connexe particulière
```{r}
list_z1_compo <- compo[id_comp == 329]$z1
input_dt <- data_rp[z1 %in% list_z1_compo]

l_area_z1_at_risk <- find_pbm_diff_tab(input_dt = input_dt,threshold = 11 )
length(l_area_z1_at_risk)

head(l_area_z1_at_risk,20)
```

299 union of elements of z1 at risk of differenciation here. let's look further one this risk area
```{r}
#lapply(l_area_z1_at_risk,length)
area_z1_at_risk <- l_area_z1_at_risk[[20]]
area_z1_at_risk

```

The return diff info function allows us to know exactly where the differencing issue occurs (which intersection square miunicipality here)
```{r}
diff_info <-return_diff_info(list_z1 = area_z1_at_risk,threshold = 11,input_dt = input_dt)
diff_info
```

On peut représenter la zone à risque de cette façon
```{r}
situation_table <- input_dt[z1 %in% area_z1_at_risk]
situation_table <- input_dt[z2 %in% situation_table$z2]
situation_table <- clean_init_dt(situation_table)

l<- preparer_geom(situation_table)

diffman::draw_situation(
  situation_table,
  l$geom_z1,
  l$geom_z2,
  list_z1_to_color = area_z1_at_risk,
  threshold = 11
               )
```

La zone jaune est la zone à risque, je fais apparaitre en bleu les communes limitrophe à la zone (au sens de la connection). Il faut bien comprendre qu'un problème de differenciation interne dans la zone jaune se caracterisera par un priobleme de differenciation externe dans le complementaire de la zone jaune dans la composante connexe.

On peut d'ailleurs faire apparaitre la totalité du complémentaire de la façon suivante : 

```{r}
situation_table <- input_dt
situation_table <- clean_init_dt(situation_table)

l<- preparer_geom(situation_table)

draw_situation(
  situation_table,
  l$geom_z1,
  l$geom_z2,
  list_z1_to_color = area_z1_at_risk,
  threshold = 11
               )

```
Ici toute la composante est représentée en tant que  zone_a_risque Union sont complémentaires dans la composante.
```{r}
diff_info
```


## The whole risk extraction process 
```{r}
## test

# list_z1_compo <- compo[id_comp %in% seq(3:50),]$z1
# input_dt <- data_rp[z1 %in% list_z1_compo]
data_rp <- data.table(readRDS("ext_data/data_rp.rds"))
input_dt <- data_rp

s <- Sys.time()
all_component_risk_extraction(
  input_dt,
  threshold = 11,
  max_agregate_size = 15,
  save_dir = "diff_info"
  )
e <- Sys.time()

e-s
```
1 h de temps dont 30 min pour la grosse composante connexe

## Lecture des résultats

```{r}

input_dt <- data_rp
global_diff_info <- read_diff_info("diff_info")
global_diff_info <- global_diff_info[!is.na(nb_obs)]
head(global_diff_info)

```

### Etude des composantes connexes
A comparer avec le nombre de communes dans chaque compo
```{r}
comp_to_n_area <- global_diff_info[,.(n_at_risk_area= length(unique(checked_area))),by ="id_comp"][rev(order(n_at_risk_area))]
comp_to_nb_com[,id_comp := as.character(id_comp)]

comp_info <- merge(comp_to_n_area,comp_to_nb_com,by ="id_comp")
comp_info[,ratio_risk_nb_com:= n_at_risk_area/nb_com]

library(ggplot2)

comp_info %>% 
  ggplot()+
  geom_point(aes(y= ratio_risk_nb_com, x = nb_com))

```

### Etude du max
```{r}
id_comp_max <- comp_info[which.max(comp_info$ratio_risk_nb_com)]$id_comp
list_z1 <- compo[id_comp == id_comp_max]$z1
input_dt <- data_rp[z1%in% list_z1]
situation_table <- clean_init_dt(input_dt)

l<- preparer_geom(situation_table)

draw_situation(
  situation_table,
  l$geom_z1,
  l$geom_z2,
  threshold = 11
               )

```
On voit pas grand chose si ce n'est que la grosse composante n'a pas tant de zones à risque ce qui doit être proche de l'idée de morcellement de la zone elle doit tenir sur quelques points faibles (peu d'intersection fragiles. La compos&ante ci-dessus a plein de lien faible d'oùù le nombre de zones à risque élevé en rapport à la taille de la composante -> forme d'indice d'urbanité.

### N checked area 
```{r}
global_diff_info$checked_area %>% unique() %>% length()
```
21272 areas à protéger dans 1902 composantes.

### N carreaux

i) à risque
```{r}
carreau_a_risque <- global_diff_info$z2 %>% unique()
length(carreau_a_risque)
```
ii) comparaison avec nb carreaux sous le seuil
```{r}
carreau_to_nb_obs <- data_rp[ ,.(nb_obs = sum(nb_obs)),by = (z2)]
carreau_au_dessus_du_seuil <- carreau_to_nb_obs[nb_obs >= 11]$z2

100*sum(carreau_a_risque %in% carreau_au_dessus_du_seuil)/length(carreau_a_risque)

```

7932 carreaux "à risque", 50 % sont au dessus du seuil des 11 ménages


## Petit laius sur la stratégie de blanchiment pour le RP

diffman va en fait extraire des intesrsections carreaux X commlunes à risque. Mais blanchir systématiquement les carreeaux impliqués dans ces intersections n'apportera pas grand chose car la différenciation interne est toujours possible même si le carreau (ou les) chevauchant est blanchi (à méditer). En fait ce qu'il faut c'est s'assurer que l'attaquant ne puisse pas faire de différenciation du tout sur la zone extraite. Comme on ne touchera jamais aux totaux communaux il s'agit en fait de s'assurer que l'attaquant nbe soit pas en mesure de reconstruire le total des carreaux totalement inclus dans la commune  (dans le cas d'une différzenciation interner ou le total des carreaux totalement inclus ou partiellement (zone de carreaux recouvrante). Pour ce faire on veut plutôt blanchir des carreaux intérieurs à la commune. et il faut aussi s'assurer que  l'agregation ne soit pas retrouvable. C'est comme si on forcait des agregations blanchies dans griddy.. PAs simple..


Idée 1 :

Gérer la différenciation du niveau le plus grossier au niveau le plus fin (ou inversement en comprenant bien le passage d'un niveau à un autre). Ce passage peut peut être s'interpréter en observant les liens entre composantes connexes niveau fin VS niveau grossier ?
Observer les différentes composantes connexes dans ce cas. Je pense qu'on tend vers la fusion.
Sinon gérer chaque niveau indépendemment. Puis récolter les masques et relier les niveaux à la manière de griddy. si un gros est blanchi il me faut au moins un autre blanchi plus bas?


Gérer ls pbs de différenciation indépendemment, peut etre qu'il y en a moins sur les niveaux grossiers. Proposer une méthode de blanchiment pour le niveau fin. Faire une analyse composante par composante

Fonction pour blanchir des carreaux au pif inf à 11 lmenages en esperant atteindre le seuil de 11
Amelioration pour blanchir des carreaux internes à la zone très influents
Amélioration de la représentationa vec les carreaux interne et les carreaux blanchis 
Puis forme d'union des masques

## Application de diffman sur les différents niveaux de la grilles superposée

Ici je vais brancher avec le travail de Julien pour observer les pbs de différenciations sur des grilles plus grosses. Même si on arrive à protéger un niveau de la différenciation comment est-on sûr que le niveau le plus fin ne donne pas d'info, sur le niveau le plus grossier ?

J'ai téléchargé le zip de griddy de gitlab.insee.fr pour avoir accès aux fonction créant les grilles superposées https://gitlab.insee.fr/outilsconfidentialite/gridy et je m'inspire ds codes de Julien dans https://gitlab.insee.fr/expertisesconfidentialite/carreaux_rp/-/blob/main/R/00_parametres.R
```{r}
remove.packages("gridy")
install.packages("fasterize")
install.packages("gridy_0.1.1.tar.gz", repos = NULL) # j'ai compilé ce package zippé sur r studio en clonant au prealable gridy sur gitlab.insee en mode archive
library(gridy)
```

Creation grille superposée avant application griddy
```{r}
input_dt <- clean_init_dt(data_rp)
# Préparation des grilles superposées
# sh_carr_1km <- carreaux_to_polygon(input_dt[1:10,], var_carreau = "z2")
table_carreau <- input_dt %>% select(z2) %>% unique()

centroid_carr_1km <- get_centroid_carreau(table_carreau, "z2")$df %>% 
  select(carreau_1km_orig = carreau, crs, x = centroid_x, y = centroid_y)

centroid_carr_1km_dt <- data.table::as.data.table(centroid_carr_1km)

mailles = 2^(0:6)*1000
names(mailles) <- paste0(2^(0:6),"km")

grilles_superposees_dt <- 
  gridy::create_grids(centroid_carr_1km_dt, mailles = mailles, eurostat = TRUE)


input_dt_grid <- input_dt %>% 
  merge(
    grilles_superposees_dt,
    by.x = "z2", by.y = "carreau_1km_orig",
  )

head(input_dt_grid)
```

On va travailler avec le niveau 2km
```{r}
# install.packages("doParallel")
# install.packages("foreach")
# library(doParallel)
# library(dplyr)

# Discuter avec Juline du passage en mode "jobs"
resolutions <- 
  c("id_carreau_niv_64km","id_carreau_niv_32km",
    "id_carreau_niv_16km","id_carreau_niv_8km",
    "id_carreau_niv_4km","id_carreau_niv_2km")
    #,"id_carreau_niv_1km") 

#s <- Sys.time()
#numCores <- 7
#cl <- parallel::makeCluster(numCores)
#doParallel::registerDoParallel(cl)
#l_risk_compo <- foreach(resolution = resolutions,
                 #       .packages=c('data.table','diffman')) %dopar% #search_diff_pb_one_resolution(resolution,input_dt_grid)
#e <- Sys.time()
vec_temps <- setNames(rep(NA,length(resolutions)),resolutions)
i = 1
for (resolution in resolutions){
  # 
  print(paste0("recherche de différenciation pour ",resolution))
  s <- Sys.time()
  print(resolution)
  search_diff_pb_one_resolution(resolution,input_dt_grid)
  e <- Sys.time()
  print(e-s)
  vec_temps[i] <- e-s
  i = i+1
  
}

# search_diff_pb_one_resolution("id_carreau_niv_2km",input_dt_grid)

e-s
```



```{r}      

global_diff_info <- read_diff_info("diff_info")

```


## Algo
C'est parti pour l'algo de blanchiement sur le niveau le plus fin :

- je prends une zone détéctée je regarde les carreauix inclus dans la zone et j'essaie dans blanchir suffisamment pour dépasser le seuil des 11 (dans l'idée ne pas choisir les intersections car on sait que ça sert à rien de les blanchir) se servir des carreaux endessous du seuil si il y en a !
- forcément en mode séquentiel pour que les autres zones pusisent beneficier des c    arreaux blanchis- mais parallelisable au sein d'une composdaznte :)
- essayer de blanchir des carreauxc cvote à cote pour l'agregation
- carte pour montrer les carreaux blanchis dans une zone pour vérifier !


Prenons l'exemple de la compo 601 et ses 4 communes
```{r}
head(global_diff_info)

# taf sur une composante
list_z1 <- compo$z1[compo$id_comp == 601]
comp_diff_info <- global_diff_info[id_comp == 601]

comp_diff_info[!duplicated(paste0(z1,z2)),]
duplicated(comp_diff_info[,c("z1","z2")])
comp_to_nb_com
# simplifier en ne s'intéressant qu'aux internes (cf l'interne est toujours l'exeterne de quelque chose d'autre)

input_dt <- data_rp[z1%in% list_z1]
situation_table <- clean_init_dt(input_dt)

l<- preparer_geom(situation_table)

draw_situation(
  situation_table,
  l$geom_z1,
  l$geom_z2,
  threshold = 11,save_name = "out"
               )

```



Premier algo : Pour chaque zone à risque on blanchit uncarreau dans la zone interne et externe en ne surblanchissant pas et en commencant par les carreaux les plus petits <11 logements

# Ok je ne vais traiter que les zones checkees pour risque interne.
A chaque fois je blanchis suffisamment dans la zone et dans son complémentazie dans la composante en me reservant la possibilité se brouiller egalement les intersections pour la partie externe !


 pour l'interne on peut pas blanchir le carreau intersectant mais pour l'externe on pourrait..
 étudier la possibilité de blanchir un carreau dans la zone interne et le carreau intersectant
 nécessiterait de retrouver le bug dans diffman..

 ok strategy , la recherche ne sort pas nécessairement tout en interne et en externe mais a priori toutest les intersections à problème ressortent :
 pour une intersetion à probleme garder une seule ligne. ET blanchir un carreau dans la zone checked et dans le complémentaire de la zone checked. dans la composante ->

Toutes les intersections à risque sortent bien de diffman mais pas forcément sous forme interne et externe 
Je n'ai besoin de traiter que les diffs interne (mais toute)
Donc je vais reconstruire ces zone si elles n 'existent pas d'où la fonction build_complete_internal_table
protect_component va blanchir un carreau dans les zones internes pour eviter la differenciation géographique

Rq :
diffman ne sort pas touts les types de differenciation par exemple la commune 26267 ne sort pas mais son complémentaire oui donc pas grave..
fonction proteger compo qui balai toutes les cheqck area blanchi de aprt et autres de la ckeked area (dans son complémentaire aussi) on blanchit jusqu'à ce que la différence dépasse 11


Application et image !!!


```{r}
# dim(global_diff_info[id_comp == 11])
# global_diff_info[id_comp == 11]$type_diff
#comp_to_nb_com
#num_comp <- 136
 num_comp <- 1232
#num_comp <- 1695 # Exemple très didactique
#num_comp <- 11 # marrant pas de problemes ici et ça marche bien 
z2_to_tag <- protect_component(num_comp,global_diff_info,data_rp)
list_z1 <- compo[id_comp == num_comp]$z1

input_dt <- clean_init_dt(data_rp[z1 %in% list_z1])
l<- preparer_geom(input_dt)

draw_situation(
  input_dt,
  l$geom_z1,
  l$geom_z2,
  threshold = 11,
  list_z2_to_color = z2_to_tag[tag == TRUE]$z2  
               ) 
#2843 3829
# sum(z2_to_tag[tag == TRUE]$z2 %in% data_rp$z2)
# sum(input_dt$z2 %in% z2_to_tag[tag == TRUE]$z2 )
# sum(l$geom_z2 %in% z2_to_tag[tag == TRUE]$z2 )
```

Maintenant je vais protéger toutes les composantes
time de la grosse compo..
```{r}
# ça prend 10 h sauvegarder ça et on en parle plus discuter de l'optimisation plus tard et du blanchiment multi niveau..
# essayer dans représenter une partie et surtout faire des stats dessus avec les autres moins longue ^^^
Sys.time()
z2_to_tag_big_compo <-protect_component(1,global_diff_info,data_rp)
Sys.time()
saveRDS(z2_to_tag_big_compo,"z2_to_tag_big_compo.rds")

# 9h40
# MC CP <3
```

```{r}
install.packages("pbapply")
library(pbapply)

# 10 min
l <- pblapply(rev(comp_to_nb_com$id_comp),function(num_comp){
 if(num_comp ==1) return(NULL)
 protect_component(num_comp,global_diff_info,data_rp)
})

res <- do.call(rbind,l)
res <-rbind(z2_to_tag_big_compo,res)

```

Des stats, nombre de carreaux blanchis, en dessus, en dessous du seuil etc..
```{r}
data_rp_tag <- merge(data_rp[,.(nb_obs = sum(nb_obs)), by ="z2"],res,by = "z2")

data_rp_tag[tag  == TRUE] # 19333 blanchis
data_rp_tag[tag  == TRUE & full_incl == FALSE] # 1242 intersections blanchies
data_rp_tag[tag  == TRUE & nb_obs >= 11] # 2304 carreaux blanchis supérieurs au seuil..

# veut-on vraiment diffuser les grilles superposées, on pourrait se servir de griddy pour ventiler mais c'est tout..

colnames(data_rp)
```
        

# Poyur la suite
- ok les intersections ne passent toujours pas revoir ce qui a changé avec code précédent
- bosser un peu la vignette
- Idée laisser les carreaux blanchis < 11 ménages à griddy, et trouver une méthode pour gérer les autres : simple si un niveau grossier est blanchi s'assurer qu'au moins un des enfants l'est. si un niveau fin est blanchi s'assurer qu'au moins (comment diffuser sur ce blanchi ?)
- par ailleurs mesure t'on la protection. apportée par la pertubation ?
- applliquer la fonction à l'ensemble des composantes connexes puis à tous les niveaux
- compter le nombre de carreaux blanchis par la méthode / le n carreaux < 11 menages
- logique de jonction multi level ?
- ratiop carreaux à risque carreaux > seuil blanchis?
- ajouter une coloration spécifique pour les carreaux blanchis au dessus du seuil !!!
- pk les intersections sont non blanchis ?
- tagger les carreaux inférieurs à 11 ménage direct ? les prendre en compte dans le calcul de ce qu'il reste à blanchir
- bien comprendre pk les facteurs c'est plus rapide
- gestion plus fine des risques pour la plus grosse composante
## TO DO :

- à réfléchir : faire les grilles superposées sans diffuser les nive    aux intermédiaires pour pouvoir appliquer diffman tranquillement ?
- intégrer la fionction diff pbm dans diffman ?
- mettre la population communale dans les communes,
- enlever le FR_unallocated de clean_init_data_diffman
- devtools::check() régler les dernieres notes(notamment diff_info repertory fantome)
- finalement on va proposer dans le package une stratégie de protection pour diffman hors gridy (ie hors multi level) et on verra ce qsu'on fait pour griddy
- Algo de protection post diffman, function qui prend en entrée la table diff_info globale  et l'input_dt qui traite compo par compo 
  i) Version Bête : je choisis des carreaux interne  < 11 ménages si ily en a assez sum( carreaux blanchis >11 alors c'est bon )
  ii) je cherche des carreaux influents (déduits de l'influence de la commune dans le graph) et les blanchis comme ça je suissûr qu'ils seront déjà utiles pour une autre zone-> découper le taf par composante ?
- parallelisation moins rigide que foreach ?
- blanchir carreaux internes aux zones à risque en sortie de diff_info
- rajouter les carreaux interne en option dans draw situation, internal z2 area param  = TRUE
- commencer déjà par gérer en intra vérifier que dans chaque zone à risque il existe au moins un carreau totalement inclus blanchis et que la somme des carreaux blanchis faiaut au moins 11
- puis enfin penser à la remontée et comment gérer ça.
- pour griddy il faut pas gérer les carreaux blanchis par diffman en voulant les protéger mais juste s'en servir et ne pas chercher d'autres cases à blanchir si il en existe déjà une, c'est une sorte de statut non sensible des cases.
- batterie de tests !
- vignette CI etc.. faire un point ave +Julien
- AU 30 VS diffman regarder les notes une fois que c'est fait
- faire des statistiques de carreaux blanchis vs carreaux sous le seuil avec le scénario où on blanchit tous les carreaux au frontière (ou un seul ?) on casse le lien en faisant ça d'ailleurs réfléchir à ce que ça veut dire ?
